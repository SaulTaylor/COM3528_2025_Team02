import audeer
import audonnx
import numpy as np
import pyaudio
import wave
import librosa
import audb
import audinterface
import pandas as pd
import joblib
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import os
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Download and Extract Model ---
url = 'https://zenodo.org/record/6221127/files/w2v2-L-robust-12.6bc4a7fd-1.1.0.zip'
cache_root = audeer.mkdir('cache')
model_root = audeer.mkdir('model')
archive_path = audeer.download_url(url, cache_root, verbose=True)
audeer.extract_archive(archive_path, model_root)
model = audonnx.load(model_root)

# --- Recording user audio ---
def recordAudio(seconds=5, sampling_rate=16000):
    chunk = 1024  # Record in chunks of 1024 samples
    sample_format = pyaudio.paInt16  # 16 bits per sample
    channels = 1
    filename = "output.wav"
    p = pyaudio.PyAudio()

    logging.info('Recording')
    stream = p.open(format=sample_format, channels=channels, rate=sampling_rate, frames_per_buffer=chunk, input=True)
    frames = []

    try:
        for _ in range(0, int(sampling_rate / chunk * seconds)):
            frames.append(stream.read(chunk))
    except Exception as e:
        logging.error(f"Error during recording: {e}")
    finally:
        stream.stop_stream()
        stream.close()
        p.terminate()

    logging.info('Finished recording')
    wf = wave.open(filename, 'wb')
    wf.setnchannels(channels)
    wf.setsampwidth(p.get_sample_size(sample_format))
    wf.setframerate(sampling_rate)
    wf.writeframes(b''.join(frames))
    wf.close()

    return filename

url = 'https://zenodo.org/record/6221127/files/w2v2-L-robust-12.6bc4a7fd-1.1.0.zip'
cache_root = audeer.mkdir('cache')
model_root = audeer.mkdir('model')

archive_path = audeer.download_url(url, cache_root, verbose=True)
audeer.extract_archive(archive_path, model_root)
model = audonnx.load(model_root)

db = audb.load(
    'emodb',
    version='1.1.1',
    format='wav',
    mixdown=True,
    sampling_rate=16000,
    full_path=False,    
    cache_root=cache_root,
    verbose=True,
)

emotion = db['emotion']['emotion'].get()
speaker = db['files']['speaker'].get()

hidden_states = audinterface.Feature(
    model.labels('hidden_states'),
    process_func=model,
    process_func_args={'outputs': 'hidden_states'},
    sampling_rate=16000,
    resample=True,
    num_workers=5,
    verbose=True,
)

features_w2v2 = hidden_states.process_index(
    emotion.index,
    root=db.root,
    cache_root=audeer.path(cache_root, 'w2v2'),
)

classifier_path = "emotion_classifier.pkl"

if not os.path.exists(classifier_path):
    clf = make_pipeline(
        StandardScaler(), 
        SVC(gamma='auto')
    )
    logo = LeaveOneGroupOut()
    truths = []
    preds = []

    for train_index, test_index in logo.split(features_w2v2, emotion, speaker):
        clf.fit(features_w2v2.iloc[train_index], emotion.iloc[train_index])
        predicted_y = clf.predict(features_w2v2.iloc[test_index])
        truths.extend(emotion.iloc[test_index])
        preds.extend(predicted_y)

    joblib.dump(clf, classifier_path)
    print("Classifier trained and saved.")
else:
    clf = joblib.load(classifier_path)
    print("Loaded existing classifier.")

def predict_emotion():
    audio_filename = recordAudio()
    signal, sr = librosa.load(audio_filename, sr=16000)  # Load the audio file

    # Assuming the processing function returns a DataFrame
    features_df = hidden_states.process_signal(signal, sr)

    # Convert DataFrame to Numpy array (assuming all columns are features)
    features_array = features_df.to_numpy()

    # Now you can safely reshape and predict
    predicted_emotion = clf.predict(features_array.reshape(1, -1))
    print(f"Predicted Emotion: {predicted_emotion[0]}")


predict_emotion()
